### Report
+ Initially, I looked at the images in the datasets and saw interference in them. I decided to process the images in the following way: for each cell with 100% saturated color component, I recalculated this component as an average of the components of neighboring cells. The result was a smoother image.
+ Next I looked at what model architectures were available in pytorch, and chose resnet18. Initially I trained the model on SGD. Then I picked up other optimizers (like Adam and QHAdam) and settled on QHAdam. For each optimizer I selected learning rate by going through it and comparing accuracy after 5 epochs of learning. resnet18, trained with QHAdam, scored about 87-88% in testing.
+ Further parameter selection didn't help me, so I started looking for other possible model options. In particular I tried resnet34 and resnet50. But since these are deeper models, they were very slow to learn. I decided that I didn't need such deep models. 
+ Then I started looking for relatively small models and found the article https://paperswithcode.com/paper/an-ensemble-of-simple-convolutional-neural, which described the models M3, M5, M7. The best result was M5, so I chose it and trained it up to 90% on validation.
+ Next I realized that model trained well for train_dataset (more than 99%) and poorly for val_dataset. So I decided to change each image in some way. To begin with I used augmenter from pytorch, it gave 91% on validation. After that I used color_jitter(torch.transforms, changes color), cat_out(self-written function that randomly chooses some small square on which values are averaged) and random_perspective(torch.transforms, randomly rotates image) instead of augmenter. Now the model was getting 91% on validation.
+ The next thing I decided to implement was an ensemble of several models. In it, we map some weight to each model, sum up the responses of the models ("probabilities" for each cluster) multiplied by the weights, and take the cluster with the maximum sum. Initially, I set all the weights equal. Such an ensemble scored 92% on validation and 91.1% on testing.
+ Next I decided to look again for model architectures. So I found CnnFnnModel, which after matching the optimizer parameters, gave 93.4% on validation and 92% on testing. 
+ At this step the problem of undertraining appeared (train_dataset was not accurate up to 99%). So I started changing the architecture of models (by changing activation functions and changing the number of layers). Changing activation functions did not give me anything. However, increasing the number of layers to M5 (increased model - M4) increased the accuracy on validation to 93%. 
+ At this step, I had accumulated enough models with 92-93% accuracy and I decided to make an ensemble again (now I first took an ensemble with the same weights, and then I tried several times to take random weights, and took the most appropriate weights). It scored 92.9% on the test.
+ I tried to adjust the ensemble coefficients by training. But every time it came down to the model that gave the best result, the coefficient was close to 1, and the rest were close to 0.
+ Next, I tried different models from pytorch (like googleNet, denseNet, mobileNet). But these models were big and took a long time to train.
+ So I decided to increase the number of CnnFnnModel layers. The increased model(CnnFnnModel_deeper) started to score 94% on validation.
+ Next, I wanted a better choice of training parameters and I was advised by the optuna library. With it I improved accuracy of the models on validation: M5 - 93.7%, M4 - 94.2%, CnnFnnModel_deeper - 95% on validation
+ Finally, I put the ensemble back together again. It scored 95.2% on validation and 93.9% on testing. 
+ Shortly before the end of the competition, I wanted to try using optune to adjust the weights for the models in the ensemble. But I implemented it after the end of the competition, and it didn't give me a better result.
+ After testing on the full set of tests the accuracy was 94.5%
